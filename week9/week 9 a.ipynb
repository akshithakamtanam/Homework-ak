{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Akshitha - Week 09 Machine Learning with Scikit-learn"
      ],
      "metadata": {
        "id": "UFWmGx_qgzqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence."
      ],
      "metadata": {
        "id": "M6aTqb4og-ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer.\n",
        "\n",
        "The best overall performance among different classification models within the Python notebook belongs to the **Random Forest model without cross-validation** (`RandomForest_noCV`) which achieved 99.93% accuracy on the training set. The training accuracy of 99.93% indicates that this model demonstrated strong memorization capabilities for the training data. A training accuracy at 99.93% is typically an indication of overfitting because such models overly match training data thus performing poorly when dealing with new unseen information.\n",
        "\n",
        "The testing accuracy of the `RandomForest_noCV` model reveals overfitting through its decline to **68.6%**. The difference between training accuracy and testing accuracy indicates that the model has become too complex because it learns random training data patterns instead of recognizing generalizable test data patterns. The testing accuracy of the **logistic regression models** (standard, L1-penalized with varying `C` values, and cross-validated models) remained consistent between **70.6% to 71.8%** across all models. The generalization between different configurations appears better because this performance remains stable while being closer to their training accuracy results.\n",
        "\n",
        "The **Grid Search CV-tuned Random Forest model** (`RandomForest_CV`) reached a better testing accuracy through employing cross-validation techniques during its hyperparameter optimization process. The training accuracy results remained high which indicates that generalization improvement occurred yet overfitting continued to affect the model performance. A Random Forest model received further tuning through `max_depth` optimization during development as `RandomForest_CV2` to minimize tree overfitting. The testing accuracy levels of this model matched those of the previous tuned model indicating that hyperparameter optimization helps yet does not eliminate the problem of overfitting when insufficient regularization exists.\n",
        "\n",
        "The Random Forest models delivered the best training accuracy but they performed similarly to logistic regression models while evaluating new data. Testing performances of **Logistic Regression model with L1 penalty (`C=10`) generated accurate results with training set precision at 73.47% and testing set precision at 71.8%.** The model presents an optimal balance between training data fit and unseen data generalization which makes it the most reliable selection according to the evidence presented."
      ],
      "metadata": {
        "id": "SQNUNDUqhIU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6cG6ADzYYMxK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_patient = pd.read_csv('./PatientAnalyticFile.csv')\n",
        "df_patient['mortality'] = np.where(df_patient['DateOfDeath'].isnull(), 0, 1)\n",
        "\n",
        "df_patient['DateOfBirth'] = pd.to_datetime(df_patient['DateOfBirth'])\n",
        "df_patient['Age_years'] = ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days / 365.25)\n",
        "\n",
        "vars_remove = ['PatientID', 'First_Appointment_Date', 'DateOfBirth', 'Last_Appointment_Date', 'DateOfDeath']\n",
        "df_patient = df_patient.drop(columns=vars_remove)\n",
        "\n",
        "df_patient = pd.get_dummies(df_patient, drop_first=True)"
      ],
      "metadata": {
        "id": "y4KAbZhChW9D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define predictors (X) and target (y)\n",
        "X = df_patient.drop('mortality', axis=1)\n",
        "y = df_patient['mortality']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Solvers to test\n",
        "solvers = ['liblinear', 'lbfgs', 'newton-cg', 'sag', 'saga']\n",
        "\n",
        "# Store results\n",
        "results = []"
      ],
      "metadata": {
        "id": "75P7PtwRhcsR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for solver in solvers:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize and fit model\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    time_taken = time.time() - start_time\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    results.append([solver, train_accuracy, test_accuracy, time_taken])\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['Solver', 'Training Accuracy', 'Testing Accuracy', 'Time Taken (seconds)'])\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NvKzrarheV7",
        "outputId": "e2956510-eded-4ff7-be4d-7725e9241e8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Solver  Training Accuracy  Testing Accuracy  Time Taken (seconds)\n",
            "0  liblinear           0.748125           0.73625              0.247304\n",
            "1      lbfgs           0.747938           0.73600              2.181107\n",
            "2  newton-cg           0.748062           0.73575              0.588134\n",
            "3        sag           0.748125           0.73625              6.544846\n",
            "4       saga           0.748125           0.73600              8.889797\n",
            "5  liblinear           0.748125           0.73625              0.049444\n",
            "6      lbfgs           0.747938           0.73600              0.521924\n",
            "7  newton-cg           0.748062           0.73575              0.370197\n",
            "8        sag           0.748125           0.73625              6.550987\n",
            "9       saga           0.748125           0.73600              8.219650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, which solver yielded the best results? Explain the basis for ranking the models - did you use training subset accuracy? Holdout subset accuracy? Time of execution? All three? Some combination of the three?\n",
        "\n",
        "\n",
        "\n",
        "The solver that delivered optimal performance according to results is **`liblinear`**. The best overall performance comes from `liblinear` based on the evaluation of testing accuracy together with training accuracy and execution time. Testing accuracy stands out as the most vital metric for evaluation because it indicates how well the model generalizes to new data it has not encountered. Training accuracy and execution time become important factors during the evaluation process of solvers alongside testing accuracy.\n",
        "\n",
        "The `liblinear` solver demonstrated testing accuracy of **0.73625** which matches the best-performing solver accuracy levels of `sag` and `saga`. The model showed strong effectiveness by reaching **0.748125** as its **training accuracy** value to indicate it captured all important training patterns. The `liblinear` solver completed its execution process in the least amount of time at **0.049444 seconds**. The execution time of `liblinear` at **0.049444 seconds** stands out as much faster than the other solvers `sag` and `saga` which needed more than **6.5 seconds** and **8.2 seconds** respectively.\n",
        "\n",
        "The execution times for `lbfgs` and `newton-cg` exceeded those of `liblinear` even though their accuracy was comparable. The execution process for `lbfgs` lasted **0.521924 seconds** yet `newton-cg` needed **0.370197 seconds** to finish. The extended run time of these solvers becomes a disadvantage since they failed to outperform the accuracy of the `liblinear` solver.\n",
        "\n",
        "The selection of the best-performing solver depends on a balanced evaluation of three factors: generalization performance through testing accuracy and speed through execution time and training accuracy. The `liblinear` solver proved itself as the top choice because it delivered the best accuracy within the shortest execution time. The logistic regression problem requires the `liblinear` solver as its optimal solution."
      ],
      "metadata": {
        "id": "2kt-T4jNhn9Q"
      }
    }
  ]
}